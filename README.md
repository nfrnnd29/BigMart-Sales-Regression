# BigMart-Sales-Regression

This code implements ensemble-learning regression to predict Item_Outlet_Sales in the BigMart dataset (sales for 1,559 products across 10 stores) using the key attributes defined in the assignment (e.g., Item_Weight, Item_Fat_Content, Item_Type, Item_MRP, Outlet_Size, and Item_Outlet_Sales), and the working file used in the code has 8,523 records and 13 columns. It follows an end-to-end pipeline: import libraries, remove the redundant index column, inspect data types (numeric vs categorical), handle missing values (including filling Item_Weight and Outlet_Size via derived outlet/item tables), perform EDA with distribution/pie/relationship plots, then convert categorical variables to numeric via labeling and one-hot encoding (including converting uint8 to int64). 

The code benchmarks basic ensembles (Linear Regression, KNN Regression, Decision Tree Regression) with aggregation methods (max voting, averaging, weighted averaging, rank averaging), then trains and tunes XGBoost (objective = loss + regularization; best reported setting: score 0.594113 with {'colsample_bytree': 0.7, 'eta': 0.1, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}), and evaluates stacking variants using MSE/RMSE-based experiments, including Voting Regressor and Stacking with Decision Tree/Random Forest/AdaBoost, plus additional blending and super-learner comparisons. The reported best overall result is XGBoost with MAE = 765.077 and R² = 0.597, slightly above the strongest simple ensemble (Rank Averaging, R² = 0.5828), and concludes that broader hyperparameter search and a preprocessing review are needed to push performance beyond ~60% R².

Check out the [Project Presentation Slides](https://docs.google.com/presentation/d/1O6AOF_mSCU8L4O9jq8fiFrtJUX-HdZMN0KAHGovRxJs/edit?usp=sharing) for more details.
